# services/index_service.py
from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Any, Tuple
import os
import faiss
import requests
import numpy as np

from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS

from dotenv import load_dotenv
load_dotenv(override=True)

# å¤ç”¨ä½ å·²æœ‰çš„æ•°æ®ç›®å½•ç»“æ„
DATA_ROOT = Path("data")

class IVFIndex:
    """åŸºäºFAISSå€’æ’ç´¢å¼•çš„æ£€ç´¢å®ç°"""
    def __init__(self, vectors: np.ndarray = None, nlist: int = 100):
        self.vectors = vectors
        self.nlist = nlist
        self.index = None
        self.dimension = None
        
        if vectors is not None:
            self.dimension = vectors.shape[1]
            # åˆ›å»ºå€’æ’ç´¢å¼•
            quantizer = faiss.IndexFlatL2(self.dimension)
            self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
            
            # è®­ç»ƒç´¢å¼•
            self.index.train(vectors)
            # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
            self.index.add(vectors)
    
    def search(self, query_vector: np.ndarray, top_k: int = 10) -> List[Tuple[int, float]]:
        """å€’æ’ç´¢å¼•æœç´¢"""
        if self.index is None:
            return []
        
        # ç¡®ä¿query_vectoræ˜¯æ­£ç¡®å½¢çŠ¶
        if len(query_vector.shape) == 1:
            query_vector = query_vector.reshape(1, -1)
        
        # æœç´¢
        scores, indices = self.index.search(query_vector, top_k)
        
        # è½¬æ¢ç»“æœä¸º(æ–‡æ¡£ID, ç›¸ä¼¼åº¦å¾—åˆ†)æ ¼å¼
        # FAISSè¿”å›çš„æ˜¯è·ç¦»ï¼Œæˆ‘ä»¬éœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦å¾—åˆ†
        results = []
        for i, (doc_id, distance) in enumerate(zip(indices[0], scores[0])):
            if doc_id != -1:  # æ’é™¤æ— æ•ˆç»“æœ
                # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
                similarity_score = 1.0 / (1.0 + distance) if distance > 0 else 1.0
                results.append((int(doc_id), float(similarity_score)))
        
        return results
    
    def search_with_docs(self, query_vector: np.ndarray, faiss_hits: List[Tuple[Document, float]], top_k: int = 10) -> List[Dict[str, Any]]:
        """IVFç´¢å¼•æ£€ç´¢å¹¶è¿”å›æ–‡æ¡£å†…å®¹"""
        if self.index is None:
            return []
        
        # æ‰§è¡ŒIVFæ£€ç´¢
        ivf_scores = self.search(query_vector, top_k)
        
        # è·å–IVFæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
        ivf_results = []
        for doc_id, ivf_score in ivf_scores:
            if doc_id < len(faiss_hits):
                doc, _ = faiss_hits[doc_id]
                ivf_results.append({
                    "doc": doc,
                    "ivf_score": ivf_score,
                    "doc_id": doc_id
                })
        
        return ivf_results
    
    def save(self, file_path: Path):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        if self.index is not None:
            faiss.write_index(self.index, str(file_path))
    
    @classmethod
    def load(cls, file_path: Path) -> 'IVFIndex':
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        index = cls()
        if file_path.exists():
            index.index = faiss.read_index(str(file_path))
            index.dimension = index.index.d
        return index

def workdir(file_id: str) -> Path:
    """è·å–å·¥ä½œç›®å½•è·¯å¾„"""
    p = DATA_ROOT / file_id
    p.mkdir(parents=True, exist_ok=True)
    return p

def markdown_path(file_id: str) -> Path:
    """è·å–Markdownæ–‡ä»¶è·¯å¾„"""
    return workdir(file_id) / "output.md"

def index_dir(file_id: str) -> Path:
    """è·å–ç´¢å¼•ç›®å½•è·¯å¾„"""
    p = workdir(file_id) / "index_faiss"
    p.mkdir(parents=True, exist_ok=True)
    return p

def ivf_index_path(file_id: str) -> Path:
    """è·å–IVFç´¢å¼•æ–‡ä»¶è·¯å¾„"""
    return index_dir(file_id) / "ivf_index.faiss"


def load_local_embeddings() -> OllamaEmbeddings: 
    """
    åŠ è½½æœ¬åœ°OllamaåµŒå…¥æ¨¡å‹
    
    Returns:
        OllamaEmbeddings: åµŒå…¥æ¨¡å‹å®ä¾‹
    
    Raises:
        Exception: å½“æ¨¡å‹åŠ è½½å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    try:
        # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
        response = requests.get("http://127.0.0.1:11434/api/tags", timeout=10)
        if response.status_code != 200:
            raise Exception("OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿Ollamaå·²å¯åŠ¨")
        
        # æ­£å¸¸åŠ è½½bge-m3æ¨¡å‹
        return OllamaEmbeddings(model="bge-m3:latest", base_url="http://127.0.0.1:11434")

    except Exception as e:
        raise Exception(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {str(e)}")


def split_markdown(md_text: str) -> List[Document]:
    # æ‹†åˆ†ç»†èŠ‚
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        # éœ€è¦æ›´ç»†å¯ä»¥åŠ  ("###", "Header 3")
    ]
    # åˆ›å»ºæ‹†åˆ†å™¨
    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    docs = splitter.split_text(md_text)
    # å¯åŠ ä¸€ç‚¹æ¸…æ´—
    cleaned: List[Document] = []
    for d in docs:
        txt = (d.page_content or "").strip()
        if not txt:
            continue
        # é™åˆ¶å¤ªé•¿çš„æ®µè½ï¼Œé¿å…å‘é‡åŒ–å‡ºé”™
        if len(txt) > 8000:
            txt = txt[:8000]
        cleaned.append(Document(page_content=txt, metadata=d.metadata))
    return cleaned

def build_faiss_index(file_id: str) -> Dict[str, Any]:
    """æ„å»ºç´¢å¼•å‘é‡çŸ¥è¯†åº“"""
    # è·å–MDæ–‡æ¡£è·¯å¾„
    md_file = markdown_path(file_id)
    if not md_file.exists():
        return {"ok": False, "error": "MARKDOWN_NOT_FOUND"}
    md_text = md_file.read_text(encoding="utf-8")
    # æ‹†åˆ†MD
    docs = split_markdown(md_text)
    if not docs:
        return {"ok": False, "error": "EMPTY_MD"}
    
    # å¯¼å…¥embeddingæ¨¡å‹
    embeddings = load_local_embeddings()
    # å­˜å…¥faisså‘é‡æ•°æ®
    vs = FAISS.from_documents(docs, embedding=embeddings)
    vs.save_local(str(index_dir(file_id)))
    
    # æ„å»ºIVFç´¢å¼•
    # è·å–æ‰€æœ‰æ–‡æ¡£çš„å‘é‡è¡¨ç¤º
    embeddings_list = []
    for doc in docs:
        # ä½¿ç”¨embeddingæ¨¡å‹è·å–æ–‡æ¡£å‘é‡
        doc_vector = embeddings.embed_query(doc.page_content)
        embeddings_list.append(doc_vector)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    embeddings_array = np.array(embeddings_list)
    
    # åˆ›å»ºIVFç´¢å¼•
    ivf_index = IVFIndex(embeddings_array, nlist=min(100, len(docs)))
    
    # ä¿å­˜IVFç´¢å¼•
    ivf_index.save(ivf_index_path(file_id))

    return {"ok": True, "chunks": len(docs)}

def calculate_rrf_scores(faiss_results: List[Tuple[Document, float]], 
                         ivf_results: List[Dict[str, Any]], 
                         k: int = 60) -> List[Tuple[Document, float]]:
    """è®¡ç®—RRFèåˆå¾—åˆ†"""
    # åˆ›å»ºæ’åæ˜ å°„
    faiss_rank_map = {}
    for rank, (doc, _) in enumerate(faiss_results):
        faiss_rank_map[doc.page_content] = rank + 1
    
    ivf_rank_map = {}
    for rank, result in enumerate(ivf_results):
        ivf_rank_map[result["doc"].page_content] = rank + 1
    
    # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
    all_docs = {}
    for doc, faiss_score in faiss_results:
        all_docs[doc.page_content] = doc
    
    for result in ivf_results:
        doc = result["doc"]
        all_docs[doc.page_content] = doc
    
    # è®¡ç®—RRFå¾—åˆ†
    rrf_scores = []
    for content, doc in all_docs.items():
        faiss_rank = faiss_rank_map.get(content, k + 1)
        ivf_rank = ivf_rank_map.get(content, k + 1)
        
        # RRFå…¬å¼: 1/(k + rank)
        rrf_score = 1.0 / (k + faiss_rank) + 1.0 / (k + ivf_rank)
        rrf_scores.append((doc, rrf_score))
    
    # æŒ‰RRFå¾—åˆ†é™åºæ’åº
    rrf_scores.sort(key=lambda x: x[1], reverse=True)
    
    return rrf_scores




def search_faiss(file_id: str, query: str, k: int = 5, use_hybrid: bool = True) -> Dict[str, Any]:
    """
    åŸºäºFAISSçš„æ··åˆæ£€ç´¢
    
    Args:
        file_id: æ–‡ä»¶ID
        query: æŸ¥è¯¢æ–‡æœ¬
        k: è¿”å›ç»“æœæ•°é‡
        use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆFAISS + IVFï¼‰
    
    Returns:
        æ£€ç´¢ç»“æœå­—å…¸
    """
    try:
        # 1. æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        idx = index_dir(file_id)
        if not (idx / "index.faiss").exists():
            return {"ok": False, "error": "INDEX_NOT_FOUND"}
        
        # 2. åŠ è½½FAISSå‘é‡åº“
        embeddings = load_local_embeddings()
        vs = FAISS.load_local(str(idx), embeddings, allow_dangerous_deserialization=True)
        
        # 3. è·å–FAISSæ£€ç´¢ç»“æœ
        faiss_hits = vs.similarity_search_with_score(query, k=k*3)
        
        # å¦‚æœä¸éœ€è¦æ··åˆæ£€ç´¢ï¼Œç›´æ¥è¿”å›FAISSç»“æœ
        if not use_hybrid:
            # è¿”å›FAISSç»“æœ
            results = []
            for i, (doc, score) in enumerate(faiss_hits[:k]):
                results.append({
                    "text": doc.page_content,
                    "score": float(score),
                    "metadata": doc.metadata,
                    "retrieval_type": "FAISS"
                })
            
            return {"ok": True, "results": results}
        
        # 4. IVFç´¢å¼•æ£€ç´¢ï¼ˆæ··åˆæ£€ç´¢æ¨¡å¼ï¼‰
        ivf_results = []
        if ivf_index_path(file_id).exists():
            try:
                ivf_index = IVFIndex.load(ivf_index_path(file_id))
                # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æˆåŠŸåŠ è½½
                if ivf_index.index is None:
                    print("IVFç´¢å¼•åŠ è½½å¤±è´¥ï¼šç´¢å¼•ä¸ºç©º")
                    ivf_results = []
                else:
                    # è·å–æŸ¥è¯¢å‘é‡
                    query_vector = embeddings.embed_query(query)
                    query_vector_np = np.array(query_vector).astype('float32')
                    
                    # ä½¿ç”¨IVFIndexç±»çš„search_with_docsæ–¹æ³•è¿›è¡Œæ£€ç´¢
                    ivf_results = ivf_index.search_with_docs(query_vector_np, faiss_hits, k*3)
                    print(f"IVFæ£€ç´¢åˆ° {len(ivf_results)} ä¸ªç»“æœ")
            except Exception as e:
                ivf_results = []
        
        # 5. ä½¿ç”¨ç‹¬ç«‹çš„RRFè®¡ç®—å‡½æ•°
        rrf_scores = calculate_rrf_scores(faiss_hits, ivf_results, k)
        print(f"RRF:{ivf_results}")
        # è¿”å›RRFèåˆç»“æœ
        results = []
        for i, (doc, rrf_score) in enumerate(rrf_scores[:k]):
            results.append({
                "text": doc.page_content,
                "score": float(rrf_score),
                "metadata": doc.metadata,
                "retrieval_type": "FAISS + IVF"
            })
        
        return {"ok": True, "results": results}
    
    except Exception as e:
        print(f"æ£€ç´¢å¤±è´¥: {e}")
        return {"ok": False, "error": f"æ£€ç´¢å¤±è´¥: {str(e)}"}

if __name__ == "__main__":

    from pathlib import Path

    path = Path(r"D:\LEARN\pythonFile\ocr_rag\data\f_6a4t8zka\index_faiss")
    print("ğŸ“ ç›®å½•å†…å®¹:", os.listdir(path))
    print("ğŸ“„ ç›®æ ‡æ–‡ä»¶:", path / "index.faiss")

    file_id = 'f_6a4t8zka'
    query = 'ç”µæ¢¯æŒ¯åŠ¨èˆ’é€‚åº¦è¯„ä¼°æ ‡å‡†'
    
    if not (path / "index.faiss").exists():
        print({"ok": False, "error": "INDEX_NOT_FOUND"})
    else:
        print("âœ… index.faiss æ–‡ä»¶å­˜åœ¨")

    # è·å–embeddingæ¨¡å‹
    embeddings = load_local_embeddings()
    vs = FAISS.load_local(str(path), embeddings, allow_dangerous_deserialization=True)
    # ç›¸ä¼¼åº¦å¾—åˆ†è®¡ç®—
    hits = vs.similarity_search_with_score(query, k=5)
    results = []
    # è¿”å›ç›¸å…³æ–‡æ¡£
    for doc, score in hits:
        results.append({
            "text": doc.page_content,
            "score": float(score),
            "metadata": doc.metadata,
        })

    print(results)
